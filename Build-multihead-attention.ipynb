{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":234240680,"sourceType":"kernelVersion"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:37.844095Z","iopub.execute_input":"2025-04-16T11:05:37.844350Z","iopub.status.idle":"2025-04-16T11:05:41.911963Z","shell.execute_reply.started":"2025-04-16T11:05:37.844331Z","shell.execute_reply":"2025-04-16T11:05:41.911106Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"inputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:41.913429Z","iopub.execute_input":"2025-04-16T11:05:41.913959Z","iopub.status.idle":"2025-04-16T11:05:41.934574Z","shell.execute_reply.started":"2025-04-16T11:05:41.913928Z","shell.execute_reply":"2025-04-16T11:05:41.933462Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"input_query = inputs[1]\ninput_query\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:41.935576Z","iopub.execute_input":"2025-04-16T11:05:41.935885Z","iopub.status.idle":"2025-04-16T11:05:42.035032Z","shell.execute_reply.started":"2025-04-16T11:05:41.935855Z","shell.execute_reply":"2025-04-16T11:05:42.033899Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"tensor([0.5500, 0.8700, 0.6600])"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"input_1 = inputs[0]\ninput_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:42.037481Z","iopub.execute_input":"2025-04-16T11:05:42.037768Z","iopub.status.idle":"2025-04-16T11:05:42.045138Z","shell.execute_reply.started":"2025-04-16T11:05:42.037744Z","shell.execute_reply":"2025-04-16T11:05:42.044190Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([0.4300, 0.1500, 0.8900])"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"query = inputs[1]\n\nattn_scores_2 = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, input_query)\n\nprint(attn_scores_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:42.046160Z","iopub.execute_input":"2025-04-16T11:05:42.046509Z","iopub.status.idle":"2025-04-16T11:05:42.074204Z","shell.execute_reply.started":"2025-04-16T11:05:42.046469Z","shell.execute_reply":"2025-04-16T11:05:42.073147Z"}},"outputs":[{"name":"stdout","text":"tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"attn_weight_2_tmp = attn_scores_2 / attn_scores_2.sum()\nattn_weight_2_tmp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:42.075101Z","iopub.execute_input":"2025-04-16T11:05:42.075426Z","iopub.status.idle":"2025-04-16T11:05:42.082643Z","shell.execute_reply.started":"2025-04-16T11:05:42.075397Z","shell.execute_reply":"2025-04-16T11:05:42.081864Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nsoftmax_naive(attn_scores_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:43.685995Z","iopub.execute_input":"2025-04-16T11:05:43.686675Z","iopub.status.idle":"2025-04-16T11:05:43.703320Z","shell.execute_reply.started":"2025-04-16T11:05:43.686648Z","shell.execute_reply":"2025-04-16T11:05:43.702441Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"attn_weights_2 = torch.softmax(attn_scores_2, dim=0) #optimize version stable but self implementation will have you understand the concept\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:43.958940Z","iopub.execute_input":"2025-04-16T11:05:43.959247Z","iopub.status.idle":"2025-04-16T11:05:43.967931Z","shell.execute_reply.started":"2025-04-16T11:05:43.959222Z","shell.execute_reply":"2025-04-16T11:05:43.966960Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#context vector: combine the information of all the input vector \n# and the those with a high attention weight will play an important role in the output ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:45.902765Z","iopub.execute_input":"2025-04-16T11:05:45.903068Z","iopub.status.idle":"2025-04-16T11:05:45.908056Z","shell.execute_reply.started":"2025-04-16T11:05:45.903045Z","shell.execute_reply":"2025-04-16T11:05:45.907074Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch.zeros(query.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.118154Z","iopub.execute_input":"2025-04-16T11:05:46.118648Z","iopub.status.idle":"2025-04-16T11:05:46.128337Z","shell.execute_reply.started":"2025-04-16T11:05:46.118619Z","shell.execute_reply":"2025-04-16T11:05:46.127211Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([0., 0., 0.])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"query = inputs[1]\n\ncontext_vec_2 = torch.zeros(query.shape)\nfor i, x_i in enumerate(inputs):\n    # print(f\"{attn_weights_2[i]} ---> {x_i}\")\n    context_vec_2 += attn_weights_2[i]*x_i\n\nprint(context_vec_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.355221Z","iopub.execute_input":"2025-04-16T11:05:46.355557Z","iopub.status.idle":"2025-04-16T11:05:46.365510Z","shell.execute_reply.started":"2025-04-16T11:05:46.355529Z","shell.execute_reply":"2025-04-16T11:05:46.364568Z"}},"outputs":[{"name":"stdout","text":"tensor([0.4419, 0.6515, 0.5683])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.3.2 A simple self-attention mechanism without trainable weights","metadata":{}},{"cell_type":"code","source":"attn_scores = torch.empty(6, 6)\n\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i, j] = torch.dot(x_i, x_j)\n\nprint(attn_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.838510Z","iopub.execute_input":"2025-04-16T11:05:46.838844Z","iopub.status.idle":"2025-04-16T11:05:46.847017Z","shell.execute_reply.started":"2025-04-16T11:05:46.838822Z","shell.execute_reply":"2025-04-16T11:05:46.846077Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"attn_scores = inputs @ inputs.T\nattn_weights = torch.softmax(attn_scores, dim=1)\nall_context_vecs = attn_weights @ inputs\nall_context_vecs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:47.106466Z","iopub.execute_input":"2025-04-16T11:05:47.106759Z","iopub.status.idle":"2025-04-16T11:05:47.131885Z","shell.execute_reply.started":"2025-04-16T11:05:47.106738Z","shell.execute_reply":"2025-04-16T11:05:47.131020Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# 3.4 Implementing self-attention with trainable weights ","metadata":{}},{"cell_type":"markdown","source":"**3.4.1 Computing the attention weights step by step**","metadata":{}},{"cell_type":"code","source":"x_2 = inputs[1]\nd_in = inputs.shape[1]\nd_out = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:49.312612Z","iopub.execute_input":"2025-04-16T11:05:49.312964Z","iopub.status.idle":"2025-04-16T11:05:49.318204Z","shell.execute_reply.started":"2025-04-16T11:05:49.312939Z","shell.execute_reply":"2025-04-16T11:05:49.317235Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"torch.manual_seed(123)\n\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out))\nW_key = torch.nn.Parameter(torch.rand(d_in, d_out))\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out))\n\nprint(f\"{W_query}\")\nprint(f\"{W_key}\")\nprint(f\"{W_value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:49.571249Z","iopub.execute_input":"2025-04-16T11:05:49.572358Z","iopub.status.idle":"2025-04-16T11:05:49.593120Z","shell.execute_reply.started":"2025-04-16T11:05:49.572319Z","shell.execute_reply":"2025-04-16T11:05:49.591732Z"}},"outputs":[{"name":"stdout","text":"Parameter containing:\ntensor([[0.2961, 0.5166],\n        [0.2517, 0.6886],\n        [0.0740, 0.8665]], requires_grad=True)\nParameter containing:\ntensor([[0.1366, 0.1025],\n        [0.1841, 0.7264],\n        [0.3153, 0.6871]], requires_grad=True)\nParameter containing:\ntensor([[0.0756, 0.1966],\n        [0.3164, 0.4017],\n        [0.1186, 0.8274]], requires_grad=True)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"query_2 = x_2 @ W_query\n\nquery_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:51.518666Z","iopub.execute_input":"2025-04-16T11:05:51.518942Z","iopub.status.idle":"2025-04-16T11:05:51.532774Z","shell.execute_reply.started":"2025-04-16T11:05:51.518923Z","shell.execute_reply":"2025-04-16T11:05:51.531900Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"keys = inputs @ W_key \nvalue = inputs @ W_value \n\nkeys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:51.719716Z","iopub.execute_input":"2025-04-16T11:05:51.720004Z","iopub.status.idle":"2025-04-16T11:05:51.727502Z","shell.execute_reply.started":"2025-04-16T11:05:51.719983Z","shell.execute_reply":"2025-04-16T11:05:51.726625Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([[0.3669, 0.7646],\n        [0.4433, 1.1419],\n        [0.4361, 1.1156],\n        [0.2408, 0.6706],\n        [0.1827, 0.3292],\n        [0.3275, 0.9642]], grad_fn=<MmBackward0>)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"keys_2 = keys[1]\nattn_score_22 =torch.dot(query_2, keys_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:52.337494Z","iopub.execute_input":"2025-04-16T11:05:52.337820Z","iopub.status.idle":"2025-04-16T11:05:52.342423Z","shell.execute_reply.started":"2025-04-16T11:05:52.337797Z","shell.execute_reply":"2025-04-16T11:05:52.341323Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"attn_score_22","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:52.615432Z","iopub.execute_input":"2025-04-16T11:05:52.615745Z","iopub.status.idle":"2025-04-16T11:05:52.622712Z","shell.execute_reply.started":"2025-04-16T11:05:52.615722Z","shell.execute_reply":"2025-04-16T11:05:52.621831Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"tensor(1.8524, grad_fn=<DotBackward0>)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"attn_scores_2 = query_2 @ keys.T\nattn_scores_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:53.158596Z","iopub.execute_input":"2025-04-16T11:05:53.158914Z","iopub.status.idle":"2025-04-16T11:05:53.166160Z","shell.execute_reply.started":"2025-04-16T11:05:53.158891Z","shell.execute_reply":"2025-04-16T11:05:53.165174Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n       grad_fn=<SqueezeBackward4>)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"d_k = keys.shape[1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nattn_weights_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:33.880243Z","iopub.execute_input":"2025-04-16T11:06:33.881233Z","iopub.status.idle":"2025-04-16T11:06:33.888430Z","shell.execute_reply.started":"2025-04-16T11:06:33.881195Z","shell.execute_reply":"2025-04-16T11:06:33.887675Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"attn_weights_2.sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:07:03.770419Z","iopub.execute_input":"2025-04-16T11:07:03.770715Z","iopub.status.idle":"2025-04-16T11:07:03.777195Z","shell.execute_reply.started":"2025-04-16T11:07:03.770692Z","shell.execute_reply":"2025-04-16T11:07:03.776428Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor(1., grad_fn=<SumBackward0>)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"context_vec_2 = attn_weights_2 @ value\n\ncontext_vec_2 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:10:26.358173Z","iopub.execute_input":"2025-04-16T11:10:26.358527Z","iopub.status.idle":"2025-04-16T11:10:26.365812Z","shell.execute_reply.started":"2025-04-16T11:10:26.358500Z","shell.execute_reply":"2025-04-16T11:10:26.364935Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"3.4.2 Implementing a compact SelfAttention class ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \n\nclass SelfAttention_v2 (nn.Module):\n\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n    \n\n    def forward(self, x):\n        queries = self.W_query(inputs)\n        keys = self.W_key(inputs)\n        values = self.W_value(inputs)\n\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n        context_vec = attn_weights @ values \n        \n        return context_vec\n\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nsa_v2(inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:50:50.862058Z","iopub.execute_input":"2025-04-16T11:50:50.862349Z","iopub.status.idle":"2025-04-16T11:50:50.874857Z","shell.execute_reply.started":"2025-04-16T11:50:50.862326Z","shell.execute_reply":"2025-04-16T11:50:50.874114Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0739,  0.0713],\n        [-0.0748,  0.0703],\n        [-0.0749,  0.0702],\n        [-0.0760,  0.0685],\n        [-0.0763,  0.0679],\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"# 3.5 Hiding future words with causal attention","metadata":{}},{"cell_type":"markdown","source":"### 3.5.1 Applying a causal attention mask","metadata":{}},{"cell_type":"code","source":"# Your journey starts with one step ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"queries = sa_v2.W_query(inputs)\nkeys = sa_v2.W_key(inputs)\nvalues = sa_v2.W_value(inputs)\n\nattn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:55:46.054871Z","iopub.execute_input":"2025-04-16T11:55:46.055156Z","iopub.status.idle":"2025-04-16T11:55:46.060673Z","shell.execute_reply.started":"2025-04-16T11:55:46.055135Z","shell.execute_reply":"2025-04-16T11:55:46.059806Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:55:49.163001Z","iopub.execute_input":"2025-04-16T11:55:49.163287Z","iopub.status.idle":"2025-04-16T11:55:49.170223Z","shell.execute_reply.started":"2025-04-16T11:55:49.163265Z","shell.execute_reply":"2025-04-16T11:55:49.169394Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"context_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:56:59.766213Z","iopub.execute_input":"2025-04-16T11:56:59.767174Z","iopub.status.idle":"2025-04-16T11:56:59.777332Z","shell.execute_reply.started":"2025-04-16T11:56:59.767139Z","shell.execute_reply":"2025-04-16T11:56:59.776434Z"}},"outputs":[{"name":"stdout","text":"tensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"masked_simple = attn_weights * mask_simple\nmasked_simple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:57:56.937979Z","iopub.execute_input":"2025-04-16T11:57:56.938526Z","iopub.status.idle":"2025-04-16T11:57:56.944779Z","shell.execute_reply.started":"2025-04-16T11:57:56.938500Z","shell.execute_reply":"2025-04-16T11:57:56.943988Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<MulBackward0>)"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"row_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:00:57.230205Z","iopub.execute_input":"2025-04-16T12:00:57.230673Z","iopub.status.idle":"2025-04-16T12:00:57.237518Z","shell.execute_reply.started":"2025-04-16T12:00:57.230643Z","shell.execute_reply":"2025-04-16T12:00:57.236540Z"}},"outputs":[{"name":"stdout","text":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<DivBackward0>)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:03:44.248010Z","iopub.execute_input":"2025-04-16T12:03:44.248289Z","iopub.status.idle":"2025-04-16T12:03:44.262197Z","shell.execute_reply.started":"2025-04-16T12:03:44.248271Z","shell.execute_reply":"2025-04-16T12:03:44.261289Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n       grad_fn=<MaskedFillBackward0>)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"### 3.5.2 Masking additional attention weights with droupout","metadata":{}},{"cell_type":"code","source":"# Applied to various layers to reduce overfiting \n# Model learn to rely less on certain postions ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(123)\n\nlayer = torch.nn.Dropout(0.5) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:39:36.139622Z","iopub.execute_input":"2025-04-16T12:39:36.139920Z","iopub.status.idle":"2025-04-16T12:39:36.145957Z","shell.execute_reply.started":"2025-04-16T12:39:36.139897Z","shell.execute_reply":"2025-04-16T12:39:36.145060Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"example = torch.ones(6,6)\nexample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:39:53.213157Z","iopub.execute_input":"2025-04-16T12:39:53.213560Z","iopub.status.idle":"2025-04-16T12:39:53.221287Z","shell.execute_reply.started":"2025-04-16T12:39:53.213530Z","shell.execute_reply":"2025-04-16T12:39:53.220434Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]])"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"layer(example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:40:00.353492Z","iopub.execute_input":"2025-04-16T12:40:00.353781Z","iopub.status.idle":"2025-04-16T12:40:00.368646Z","shell.execute_reply.started":"2025-04-16T12:40:00.353763Z","shell.execute_reply":"2025-04-16T12:40:00.367856Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"tensor([[2., 2., 2., 2., 2., 2.],\n        [0., 2., 0., 0., 0., 0.],\n        [0., 0., 2., 0., 2., 0.],\n        [2., 2., 0., 0., 0., 2.],\n        [2., 0., 0., 0., 0., 2.],\n        [0., 2., 0., 0., 0., 0.]])"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"dropout_rate = 0.5\n1 / (1 - dropout_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:41:04.050776Z","iopub.execute_input":"2025-04-16T12:41:04.051057Z","iopub.status.idle":"2025-04-16T12:41:04.057048Z","shell.execute_reply.started":"2025-04-16T12:41:04.051039Z","shell.execute_reply":"2025-04-16T12:41:04.055918Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"2.0"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"layer(attn_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:42:02.996249Z","iopub.execute_input":"2025-04-16T12:42:02.996634Z","iopub.status.idle":"2025-04-16T12:42:03.005219Z","shell.execute_reply.started":"2025-04-16T12:42:02.996600Z","shell.execute_reply":"2025-04-16T12:42:03.004322Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"tensor([[0.3843, 0.3293, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.3324, 0.0000, 0.0000, 0.2955],\n        [0.0000, 0.0000, 0.3325, 0.2996, 0.0000, 0.2961],\n        [0.0000, 0.3334, 0.0000, 0.3142, 0.3322, 0.0000],\n        [0.0000, 0.3337, 0.3339, 0.3177, 0.3317, 0.3169],\n        [0.3869, 0.3327, 0.0000, 0.0000, 0.3331, 0.3058]],\n       grad_fn=<MulBackward0>)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.5.3 Implementing a compact causal self-attention class","metadata":{}},{"cell_type":"code","source":"batch = torch.stack((inputs, inputs), dim=0)\nbatch.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:44:25.990248Z","iopub.execute_input":"2025-04-16T12:44:25.991059Z","iopub.status.idle":"2025-04-16T12:44:26.003316Z","shell.execute_reply.started":"2025-04-16T12:44:25.991028Z","shell.execute_reply":"2025-04-16T12:44:26.002480Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 6, 3])"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"import torch.nn as nn \n\nclass CausalAttention (nn.Module):\n\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n    \n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        \n        queries = self.W_query(x)\n        keys = self.W_key(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n        attn_scores.masked_fill_(\n            self.mask.bool()[:num_tokens, :num_tokens],-torch.inf)\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n        attn_weights = self.dropout(attn_weights)\n        context_vec = attn_weights @ values \n        \n        return context_vec\n\ntorch.manual_seed(123)\ncontext_length=batch.shape[1]\ndropout = 0.0\nca = CausalAttention(d_in, d_out, context_length, dropout)\ncontext_vecs = ca(batch)\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:59:07.955151Z","iopub.execute_input":"2025-04-16T12:59:07.955537Z","iopub.status.idle":"2025-04-16T12:59:07.968165Z","shell.execute_reply.started":"2025-04-16T12:59:07.955512Z","shell.execute_reply":"2025-04-16T12:59:07.967429Z"}},"outputs":[{"name":"stdout","text":"tensor([[[-0.4519,  0.2216],\n         [-0.5874,  0.0058],\n         [-0.6300, -0.0632],\n         [-0.5675, -0.0843],\n         [-0.5526, -0.0981],\n         [-0.5299, -0.1081]],\n\n        [[-0.4519,  0.2216],\n         [-0.5874,  0.0058],\n         [-0.6300, -0.0632],\n         [-0.5675, -0.0843],\n         [-0.5526, -0.0981],\n         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"batch ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:00:50.394052Z","iopub.execute_input":"2025-04-16T13:00:50.394362Z","iopub.status.idle":"2025-04-16T13:00:50.402203Z","shell.execute_reply.started":"2025-04-16T13:00:50.394329Z","shell.execute_reply":"2025-04-16T13:00:50.401216Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"tensor([[[0.4300, 0.1500, 0.8900],\n         [0.5500, 0.8700, 0.6600],\n         [0.5700, 0.8500, 0.6400],\n         [0.2200, 0.5800, 0.3300],\n         [0.7700, 0.2500, 0.1000],\n         [0.0500, 0.8000, 0.5500]],\n\n        [[0.4300, 0.1500, 0.8900],\n         [0.5500, 0.8700, 0.6600],\n         [0.5700, 0.8500, 0.6400],\n         [0.2200, 0.5800, 0.3300],\n         [0.7700, 0.2500, 0.1000],\n         [0.0500, 0.8000, 0.5500]]])"},"metadata":{}}],"execution_count":73},{"cell_type":"markdown","source":"# 3.6 Extending single-head attention to multi-head attention ","metadata":{}},{"cell_type":"markdown","source":"## 3.6.1 Stacking multiple single-head attention layers ","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads=2, qkv_bias=False):\n        super().__init__()\n        self.heads= nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias=False) for _ in range(num_heads)\n                                  ])\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n\ntorch.manual_seed(123)\ncontext_length = batch.shape[1]\nd_in, d_out = 3, 2\n\nmha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2)\nmha(batch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:22:15.570131Z","iopub.execute_input":"2025-04-16T13:22:15.570469Z","iopub.status.idle":"2025-04-16T13:22:15.583152Z","shell.execute_reply.started":"2025-04-16T13:22:15.570442Z","shell.execute_reply":"2025-04-16T13:22:15.582356Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\n\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"### 3.6.1 Implementing multi-head attention with splits ","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n        # this will result in errors in the mask creation further below. \n        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n        # do not exceed `context_length` before reaching this forwar\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n        \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2) \n        \n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec\n\ntorch.manual_seed(123)\n\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:40:32.277424Z","iopub.execute_input":"2025-04-16T13:40:32.277740Z","iopub.status.idle":"2025-04-16T13:40:32.293880Z","shell.execute_reply.started":"2025-04-16T13:40:32.277716Z","shell.execute_reply":"2025-04-16T13:40:32.293155Z"}},"outputs":[{"name":"stdout","text":"tensor([[[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]],\n\n        [[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}